{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khawar-khan520/nlp_project/blob/main/retrieval_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "445f2f8a",
      "metadata": {
        "id": "445f2f8a"
      },
      "source": [
        "Install and Import Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71010369",
      "metadata": {
        "id": "71010369"
      },
      "outputs": [],
      "source": [
        "!pip install openai sentence-transformers faiss-cpu hf_xet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21deb282",
      "metadata": {
        "id": "21deb282"
      },
      "source": [
        "Load and Chunk your Document:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "Rauu0rlNP06N"
      },
      "id": "Rauu0rlNP06N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40d6da7",
      "metadata": {
        "id": "d40d6da7"
      },
      "outputs": [],
      "source": [
        "\n",
        "with open('winnie_the_pooh.txt', 'r') as file:\n",
        "    # Read the entire content of the file into a string\n",
        "    text = file.read()\n",
        "\n",
        "chunks = [text[i:i+200] for i in range(0, len(text), 200)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f06cd075",
      "metadata": {
        "id": "f06cd075"
      },
      "source": [
        "Generate Embeddings with SenteceTransformers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f757a9",
      "metadata": {
        "id": "64f757a9"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Take a sample of 10 chunks\n",
        "sample_embeddings = embeddings[:10]\n",
        "similarity_matrix = cosine_similarity(sample_embeddings)\n",
        "\n",
        "# Print the similarity matrix\n",
        "print(np.round(similarity_matrix, 2))\n"
      ],
      "metadata": {
        "id": "ve5rhug2SywF"
      },
      "id": "ve5rhug2SywF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(sample_embeddings)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(reduced[:, 0], reduced[:, 1])\n",
        "for i, chunk in enumerate(chunks[:10]):\n",
        "    plt.annotate(f\"Chunk {i}\", (reduced[i, 0], reduced[i, 1]))\n",
        "plt.title(\"PCA of Text Embeddings\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3PVTgICVS12-"
      },
      "id": "3PVTgICVS12-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "22a935f0",
      "metadata": {
        "id": "22a935f0"
      },
      "source": [
        "Store Embeddings in a FAISS Index for Similarity Search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5655044b",
      "metadata": {
        "id": "5655044b"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "index = faiss.IndexFlatL2(embeddings[0].shape[0])\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "# Search\n",
        "query = \"Who is always sad?\"\n",
        "query_embedding = model.encode([query])\n",
        "D, I = index.search(np.array(query_embedding), k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49880ec2",
      "metadata": {
        "id": "49880ec2"
      },
      "outputs": [],
      "source": [
        "for i in I[0]:\n",
        "    print(chunks[i])\n",
        "    print(\"....\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a86dc2",
      "metadata": {
        "id": "88a86dc2"
      },
      "source": [
        "Build the Prompt from Retrieved Chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebce8464",
      "metadata": {
        "id": "ebce8464"
      },
      "outputs": [],
      "source": [
        "\n",
        "retrieved_chunks = [chunks[i] for i in I[0]]\n",
        "\n",
        "# Format the prompt\n",
        "context = \"\\n\\n\".join(retrieved_chunks)\n",
        "#query = \"What is the capital of France?\"\n",
        "\n",
        "prompt = f\"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b06930",
      "metadata": {
        "id": "50b06930"
      },
      "source": [
        "Generate an Answer Using a Lightweight Language Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f117f4",
      "metadata": {
        "id": "d4f117f4"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Load a small, instruction-tuned model\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Build prompt from chunks\n",
        "retrieved_chunks = [chunks[i] for i in I[0]]\n",
        "context = \"\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "\n",
        "# Simple instruction-style prompt for T5\n",
        "prompt = f\"Answer the question based on the context.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{query}\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "# Decode and print\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Observations\n",
        "\n",
        "- **Query 1**: \"Who is Winnie the Pooh?\" - Retrieved top-k chunks focus on Pooh's identity and characteristics.\n",
        "- **Query 2**: \"Tell me about Pooh.\" - Retrieved chunks include more **descriptive information** about Pooh.\n",
        "- **Query 3**: \"What is the plot of Winnie the Pooh?\" - Retrieved chunks contain more of the **story** and overall plot.\n",
        "- **Query 4**: \"Who is the protagonist in the story?\" - Similar to Query 1, but retrieved chunks could be **more focused on his role in the story**.\n",
        "\n",
        "The differences in performance happen because each query targets slightly different aspects of the text, leading FAISS to retrieve different chunks.\n"
      ],
      "metadata": {
        "id": "KfPGtYf8XUdC"
      },
      "id": "KfPGtYf8XUdC"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}